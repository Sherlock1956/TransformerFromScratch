# Transformer Language Model from Scratch

åœ¨llm codingå’Œvibe codingç››è¡Œçš„æ—¶ä»£ï¼Œé€æ¸æ„Ÿè§‰åˆ°è‡ªå·±çš„ä»£ç èƒ½åŠ›è¶Šæ¥è¶Šå¼±ï¼ŒåŒæ—¶æœ¬ç€æƒ³æ·±å…¥å­¦ä¹ ä¸€ä¸ªåŸºç¡€çš„å¤§æ¨¡å‹æ˜¯å¦‚ä½•æ„å»ºæˆçš„ï¼Œæ‰€ä»¥æœ¬é¡¹ç›®è¯ç”Ÿäº†ã€‚

é¡¹ç›®å®Œæˆåå‘ç°æ‰‹å†™ä¸€ä¸ªå®Œæ•´çš„å¤§æ¨¡å‹å’Œè®­ç»ƒè¿‡ç¨‹å¹¶ä¸å¤æ‚ï¼Œå¸Œæœ›å¯ä»¥å¸®åŠ©æ›´å¤šçš„äººå­¦ä¹ å¤§æ¨¡å‹ã€‚

æ¬¢è¿å¯¹æœ¬äººçš„ä»£ç æ‰¹è¯„æŒ‡æ­£ï¼Œæ¬¢è¿æå‡ºGithub Issue / Github PR : )

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå‚è€ƒCS336è¯¾ç¨‹ï¼Œåœ¨å…è®¸ä½¿ç”¨Pytorchä¸­çš„åŸºç¡€åº“çš„æƒ…å†µä¸‹ï¼Œä»é›¶å¼€å§‹æ‰‹å†™å®ç°çš„Transformerå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬äº†ä»¥ä¸‹å†…å®¹ï¼š

- åˆ†è¯å™¨ï¼ŒTokenizer
  - BPEåˆ†è¯å™¨è®­ç»ƒ
  - Tokenizerä¿å­˜ä¸åŠ è½½
- å¤§æ¨¡å‹ç»“æ„
  - pre-rmsnorm
  - RoPE
  - SwiGLU
- æ¨¡å‹è®­ç»ƒ
  - Cross EntropyæŸå¤±å‡½æ•°
  - AdamWä¼˜åŒ–å™¨
  - cosine_scheduleå­¦ä¹ ç‡æ§åˆ¶
  - gradient_norm_clippingæ¨¡å‹æ¢¯åº¦æ§åˆ¶
- æ¨¡å‹æ¨ç†
  - æ¸©åº¦é‡‡æ ·ï¼Œæ ¸é‡‡æ ·å®ç°
  - åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹æŸå¤±å‡½æ•°

è¯¥é¡¹ç›®ç‰¹åˆ«å…³æ³¨åœ¨TinyStoriesæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒå’Œæµ‹è¯•ï¼Œå¹¶å¯¹æ¨¡å‹ç»“æ„å’Œè¶…å‚æ•°è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒå’Œåˆ†æï¼Œå…·ä½“åˆ†ææ–‡æ¡£è§test_logsï¼ŒåŒ…æ‹¬ï¼š

- æ¢ç©¶æ¨¡å‹çš„ä¸åŒè¶…å‚æ•°å¯¹æ¨¡å‹è®­ç»ƒç»“æœçš„å½±å“
- æ¢ç©¶ä¸åŒçš„æ¨¡å‹ç»“æ„ï¼ˆLayer normä½ç½®ï¼ŒRoPEï¼ŒSwiGLU or SiLUç­‰ï¼‰å¯¹æ¨¡å‹è®­ç»ƒç»“æœçš„å½±å“

## é¡¹ç›®ç»“æ„
```
TransformerFromScratch/
â”œâ”€â”€ modules/                    # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ modules.py             # æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å®ç°
â”œâ”€â”€ train/                     # è®­ç»ƒç›¸å…³è„šæœ¬
â”‚   â”œâ”€â”€ config.py             # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ train_my_bpe.py       # BPEåˆ†è¯å™¨è®­ç»ƒ
â”‚   â”œâ”€â”€ tokenize_corpus.py    # è¯­æ–™åº“åˆ†è¯
â”‚   â””â”€â”€ train_my_lm.py        # è¯­è¨€æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ test/                      # æµ‹è¯•å’Œæ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ test_my_lm.py         # æ¨¡å‹æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
â”‚   â””â”€â”€ val_my_lm.py          # æ¨¡å‹éªŒè¯è¯„ä¼°
â”œâ”€â”€ data/                      # æ•°æ®æ–‡ä»¶å¤¹
â”‚   â”œâ”€â”€ TinyStoriesV2-GPT4-train.txt
â”‚   â”œâ”€â”€ TinyStoriesV2-GPT4-valid.txt
â”‚   â””â”€â”€ *.tokenized.npy       # åˆ†è¯åçš„æ•°æ®
â”œâ”€â”€ models/                    # æ¨¡å‹ä¿å­˜ç›®å½•
â””â”€â”€ test_logs/                # å®éªŒæ—¥å¿—
```

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. BPEåˆ†è¯å™¨ (`My_tokenizer`)

å®ç°äº†å®Œæ•´çš„BPEï¼ˆByte Pair Encodingï¼‰åˆ†è¯ç®—æ³•ï¼š

- **é¢„åˆ†è¯å¤„ç†**ï¼šæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯å’Œç‰¹æ®Šæ ‡è®°å¤„ç†
- **å¹¶è¡ŒåŒ–è®­ç»ƒ**ï¼šä½¿ç”¨å¤šè¿›ç¨‹åŠ é€ŸBPEè®­ç»ƒè¿‡ç¨‹
- **ç‰¹æ®Šæ ‡è®°æ”¯æŒ**ï¼šæ­£ç¡®å¤„ç†`<|endoftext|>`ç­‰ç‰¹æ®Šæ ‡è®°
- **ç¼–ç /è§£ç **ï¼šæ”¯æŒæ–‡æœ¬åˆ°token IDçš„åŒå‘è½¬æ¢

æ ¸å¿ƒç‰¹æ€§ï¼š
```python
# åˆ†è¯å™¨åˆå§‹åŒ–
tokenizer = My_tokenizer(special_tokens=['<|endoftext|>'])

# æ–‡æœ¬ç¼–ç 
token_ids = tokenizer.encode("Hello, world!")

# è§£ç è¿˜åŸ
text = tokenizer.decode(token_ids)
```

### 2. Transformeræ¶æ„ç»„ä»¶

#### åŸºç¡€å±‚
- **`My_Linear`**: è‡ªå®šä¹‰çº¿æ€§å±‚ï¼Œä½¿ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
- **`My_Embedding`**: è¯åµŒå…¥å±‚
- **`My_rmsnorm`**: RMSNormå½’ä¸€åŒ–å±‚ï¼Œç›¸æ¯”LayerNormæ›´ç¨³å®š

#### æ³¨æ„åŠ›æœºåˆ¶
- **`My_RoPE`**: æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddingï¼‰
- **`My_scaled_dot_product_attention`**: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
- **`My_multihead_attention`**: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œé›†æˆRoPEä½ç½®ç¼–ç 

#### å‰é¦ˆç½‘ç»œ
- **`My_SwiGLU`**: SwiGLUæ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œ
- **`My_SiLU`**: Swishæ¿€æ´»å‡½æ•°å®ç°

#### å®Œæ•´æ¨¡å‹
- **`My_transformer_block`**: å•ä¸ªTransformerå±‚ï¼ˆæ³¨æ„åŠ›+å‰é¦ˆ+æ®‹å·®è¿æ¥ï¼‰
- **`My_transformer_lm`**: å®Œæ•´çš„è¯­è¨€æ¨¡å‹æ¶æ„

### 3. è®­ç»ƒç»„ä»¶

#### ä¼˜åŒ–å™¨
- **`My_AdamW`**: AdamWä¼˜åŒ–å™¨å®ç°ï¼Œæ”¯æŒæƒé‡è¡°å‡

#### å­¦ä¹ ç‡è°ƒåº¦
- **`My_lr_cosine_schedule`**: ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼Œæ”¯æŒwarmup

#### è®­ç»ƒå·¥å…·
- **`My_cross_entropy`**: æ•°å€¼ç¨³å®šçš„äº¤å‰ç†µæŸå¤±
- **`My_gradient_clipping`**: æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **`My_get_batch`**: é«˜æ•ˆçš„æ‰¹æ¬¡æ•°æ®é‡‡æ ·
- **`My_save_checkpoint`/`My_load_checkpoint`**: æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½

## ä½¿ç”¨æ–¹æ³•

### 1. ç¯å¢ƒé…ç½®

- æ¨èä½¿ç”¨uvç®¡ç†é¡¹ç›®ç¯å¢ƒ

```bash
# ä½¿ç”¨uvç®¡ç†é¡¹ç›®è™šæ‹Ÿç¯å¢ƒ
cd TransformerFromScratch
uv sync
source .venv/bin/activate
```

- ä½¿ç”¨condaç®¡ç†é¡¹ç›®ç¯å¢ƒ

```bash
# ä½¿ç”¨condaç®¡ç†é¡¹ç›®è™šæ‹Ÿç¯å¢ƒ
cd TransformerFromScratch
conda create --name transformer_lm python=3.12
conda activate transformer_lm
pip install .
```

- æ•°æ®é›†ä¸‹è½½

ä½¿ç”¨TinyStoriesæ•°æ®é›†ï¼š

**è®­ç»ƒé›†**: 2.12M tokens

**éªŒè¯é›†**: 22K tokens

TinyStoriesæ˜¯ä¸“é—¨ä¸ºå°å‹è¯­è¨€æ¨¡å‹è®¾è®¡çš„æ•°æ®é›†ï¼ŒåŒ…å«ç®€å•çš„å„¿ç«¥æ•…äº‹ï¼Œé€‚åˆç”¨ä½œç®€å•æ¨¡å‹çš„é¢„è®­ç»ƒè¯­æ–™ã€‚

```bash
mkdir -p data
cd data

wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

cd ..
```

### 2. è®­ç»ƒBPEåˆ†è¯å™¨

```bash
python train/train_my_bpe.py
```

è¿™å°†åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ª10000è¯æ±‡é‡çš„BPEåˆ†è¯å™¨ã€‚

### 3. æ•°æ®é¢„å¤„ç†

```bash
python train/tokenize_corpus.py
```

å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—å¹¶ä¿å­˜ä¸ºnumpyæ•°ç»„ã€‚

### 4. è®­ç»ƒè¯­è¨€æ¨¡å‹

```bash
python train/train_my_lm.py
```

ä½¿ç”¨config.pyé…ç½®æ–‡ä»¶ä¸­çš„è¶…å‚æ•°è®­ç»ƒTransformerè¯­è¨€æ¨¡å‹ã€‚

### 5. æ¨¡å‹æµ‹è¯•

```bash
python test/test_my_lm.py    # æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼Œå¯è‡ªå®šæ–‡æœ¬å‰ç¼€ç”Ÿæˆåç»­æ–‡æœ¬
python test/val_my_lm.py     # åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•äº¤å‰ç†µæŸå¤±
```

âš ï¸ Academic Honesty Notice  
This repository contains my solutions to assignments for CS336 (2025).  
It is shared **for educational and reference purposes only**.

- âœ… You are welcome to:  
  - Study the code to understand concepts  
  - Run experiments locally  
  - Cite this work (with attribution)  

- âŒ Please do **NOT**:  
  - Submit this code (or minor modifications) as your own coursework  
  - Use it to violate your institution's academic integrity policy  

If you're taking CS336 (or similar), try solving problems yourself first â€” youâ€™ll learn more! ğŸ˜Š

